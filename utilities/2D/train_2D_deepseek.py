import os
from time import time
import torch
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader, random_split
from dataset.dataset2D import SliceDataset
from loss.Dice import DiceLoss
from loss.ELDice import ELDiceLoss
from loss.WBCE import WCELoss
from loss.Jaccard import JaccardLoss
from loss.SS import SSLoss
from loss.Tversky import TverskyLoss
from loss.Hybrid import HybridLoss
from loss.BCE import BCELoss
from net.ResUNet2D import net
import parameter as para

os.environ['CUDA_VISIBLE_DEVICES'] = para.gpu
cudnn.benchmark = para.cudnn_benchmark

if __name__ == '__main__':
    # åˆå§‹åŒ–ç½‘ç»œ
    net = torch.nn.DataParallel(net).cuda()
    net.train()

    # 1. åŠ è½½å®Œæ•´è®­ç»ƒæ•°æ®é›†å¹¶åˆ’åˆ†
    full_dataset = SliceDataset(os.path.join(para.training_set_2d_path, 'ct'), os.path.join(para.training_set_2d_path, 'seg'))

    # è®¡ç®—åˆ’åˆ†å¤§å° (80% è®­ç»ƒ, 20% éªŒè¯)
    dataset_size = len(full_dataset)
    train_size = int(0.8 * dataset_size)
    val_size = dataset_size - train_size

    # éšæœºåˆ’åˆ†æ•°æ®é›†
    train_dataset, val_dataset = random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    )

    print(f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {train_size} æ ·æœ¬, éªŒè¯é›† {val_size} æ ·æœ¬")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dl = DataLoader(train_dataset, para.batch_size, True, num_workers=para.num_workers, pin_memory=para.pin_memory)
    val_dl = DataLoader(val_dataset, para.batch_size, False, num_workers=para.num_workers, pin_memory=para.pin_memory)

    # 2. åˆå§‹åŒ–æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    loss_func_list = [DiceLoss(), ELDiceLoss(), WCELoss(), JaccardLoss(), SSLoss(), TverskyLoss(), HybridLoss(), BCELoss()]
    loss_func = loss_func_list[5]  # ä½¿ç”¨TverskyLoss
    opt = torch.optim.Adam(net.parameters(), lr=para.learning_rate)
    lr_decay = torch.optim.lr_scheduler.MultiStepLR(opt, para.learning_rate_decay)
    alpha = para.alpha

    # 3. åˆå§‹åŒ–æ—©åœç›¸å…³å˜é‡
    best_val_loss = float('inf')
    patience_counter = 0
    best_epoch = 0
    best_model_weights = None

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs('../module', exist_ok=True)
    os.makedirs('../../weight', exist_ok=True)

    start = time()
    print("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(para.Epoch):
        # è®­ç»ƒé˜¶æ®µ
        net.train()
        lr_decay.step()
        mean_loss = []
        train_losses = []

        # è®­ç»ƒå¾ªç¯
        for step, (ct, seg) in enumerate(train_dl):
            ct = ct.cuda()
            seg = seg.cuda()
            outputs = net(ct)
            loss1 = loss_func(outputs[0], seg)
            loss2 = loss_func(outputs[1], seg)
            loss3 = loss_func(outputs[2], seg)
            loss4 = loss_func(outputs[3], seg)
            loss = (loss1 + loss2 + loss3) * alpha + loss4

            mean_loss.append(loss4.item())
            train_losses.append(loss.item())

            opt.zero_grad()
            loss.backward()
            opt.step()

            if step % 1000 == 0 and step > 0:
                print('epoch:{}, step:{}, loss1:{:.3f}, loss2:{:.3f}, loss3:{:.3f}, loss4:{:.3f}, time:{:.3f} min'.format(
                    epoch, step, loss1.item(), loss2.item(), loss3.item(), loss4.item(), (time() - start) / 60))

        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        current_train_loss = sum(train_losses) / len(train_losses)
        current_train_loss4 = sum(mean_loss) / len(mean_loss)

        # éªŒè¯é˜¶æ®µ
        net.eval()
        val_losses = []
        val_losses4 = []

        with torch.no_grad():
            for val_ct, val_seg in val_dl:
                val_ct = val_ct.cuda()
                val_seg = val_seg.cuda()
                val_outputs = net(val_ct)

                # è®¡ç®—æ€»éªŒè¯æŸå¤±å’Œä¸»æŸå¤±
                v_loss1 = loss_func(val_outputs[0], val_seg)
                v_loss2 = loss_func(val_outputs[1], val_seg)
                v_loss3 = loss_func(val_outputs[2], val_seg)
                v_loss4 = loss_func(val_outputs[3], val_seg)
                v_total_loss = (v_loss1 + v_loss2 + v_loss3) * alpha + v_loss4

                val_losses.append(v_total_loss.item())
                val_losses4.append(v_loss4.item())

        current_val_loss = sum(val_losses) / len(val_losses)
        current_val_loss4 = sum(val_losses4) / len(val_losses4)

        # è¾“å‡ºè®­ç»ƒå’ŒéªŒè¯ä¿¡æ¯
        print(f'Epoch {epoch}: Train Loss: {current_train_loss:.4f} (ä¸»æŸå¤±: {current_train_loss4:.4f}), '
              f'Val Loss: {current_val_loss:.4f} (ä¸»æŸå¤±: {current_val_loss4:.4f}), '
              f'LR: {opt.param_groups[0]["lr"]:.6f}')

        # 4. æ—©åœæœºåˆ¶é€»è¾‘
        if current_val_loss < best_val_loss:
            # éªŒè¯æŸå¤±æå‡ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹
            improvement = best_val_loss - current_val_loss
            best_val_loss = current_val_loss
            best_epoch = epoch
            patience_counter = 0  # é‡ç½®è€å¿ƒè®¡æ•°å™¨

            # ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡
            best_model_weights = net.state_dict().copy()
            torch.save(best_model_weights, f'./module/best_model_epoch_{epoch}_val_loss_{current_val_loss:.4f}.pth')

            print(f'âœ… éªŒè¯æŸå¤±æå‡: {improvement:.4f}, ä¿å­˜æœ€ä½³æ¨¡å‹ (epoch {epoch})')
        else:
            # éªŒè¯æŸå¤±æœªæå‡
            patience_counter += 1
            degradation = current_val_loss - best_val_loss
            print(f'âŒ éªŒè¯æŸå¤±æœªæå‡ (+{degradation:.4f}), è€å¿ƒè®¡æ•°: {patience_counter}/{para.patience}')

            # æ£€æŸ¥æ˜¯å¦è§¦å‘æ—©åœ
            if patience_counter >= para.patience:
                print(f'ğŸ›‘ æ—©åœè§¦å‘! è¿ç»­ {para.patience} ä¸ªepochéªŒè¯æŸå¤±æœªæå‡')
                print(f'æœ€ä½³æ¨¡å‹åœ¨ epoch {best_epoch}, éªŒè¯æŸå¤±: {best_val_loss:.4f}')

                # æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡
                if best_model_weights is not None:
                    net.load_state_dict(best_model_weights)
                    final_save_path = f'./module/final_best_model_epoch_{best_epoch}_val_loss_{best_val_loss:.4f}.pth'
                    torch.save(net.state_dict(), final_save_path)
                    print(f'ğŸ’¾ å·²æ¢å¤å¹¶ä¿å­˜æœ€ç»ˆæœ€ä½³æ¨¡å‹: {final_save_path}')
                else:
                    print('âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æƒé‡')

                break  # è·³å‡ºè®­ç»ƒå¾ªç¯

        # 5. æŒ‰å‘¨æœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå¯é€‰ï¼‰
        if epoch % 50 == 0 and epoch != 0:
            checkpoint_path = f'./weight/2Dnet_epoch{epoch}_train{current_train_loss:.4f}_val{current_val_loss:.4f}.pth'
            torch.save(net.state_dict(), checkpoint_path)
            print(f'ğŸ“ ä¿å­˜å‘¨æœŸæ£€æŸ¥ç‚¹: {checkpoint_path}')

        # 6. è¡°å‡æ·±åº¦ç›‘ç£ç³»æ•° alpha
        if epoch % 40 == 0 and epoch != 0:
            old_alpha = alpha
            alpha *= 0.8
            print(f'ğŸ”§ æ·±åº¦ç›‘ç£ç³»æ•°è¡°å‡: {old_alpha:.3f} -> {alpha:.3f}')

    # 7. è®­ç»ƒå®Œæˆåçš„å¤„ç†
    if epoch == para.Epoch - 1:
        print(f'ğŸ‰ è®­ç»ƒå®Œæˆ! å…±è®­ç»ƒ {para.Epoch} ä¸ªepoch')
        if best_model_weights is not None:
            print(f'æœ€ä½³æ¨¡å‹åœ¨ epoch {best_epoch}, éªŒè¯æŸå¤±: {best_val_loss:.4f}')
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_path = f'./weight/final_model_epoch{best_epoch}_val_loss{best_val_loss:.4f}.pth'
            torch.save(best_model_weights, final_path)
            print(f'ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}')

    total_time = (time() - start) / 60
    print(f'â° æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f} åˆ†é’Ÿ')
    print(f'ğŸ† æœ€ä½³ç»“æœ - Epoch: {best_epoch}, éªŒè¯æŸå¤±: {best_val_loss:.4f}')